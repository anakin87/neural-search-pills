# Retrieval-based Answer Generation

Unlike [Extractive systems](machine-reading-at-scale.md), **Generative/Abstractive Question Answering** systems create answers to questions, based on some knowledge.
The answers generated by abstractive systems can aggregate information contained in multiple original passages and are human-like.

## Generative closed-book üìï systems (ChatGPT-like)

Today everybody knows and talks about ChatGPT.
If we look at it from the Question Answering point of view, it is a **closed-book** system: it is based on its internal knowledge.
This knowledge is also known as "parametric memory": it is stored in the model weights and is accumulated during training.

When used in isolation, abstractive closed-book QA systems have some serious limitations:

‚ùå their knowledge does not fit a specific domain and is expensive and difficult to update over time

‚ùå they can produce "hallucinations"

## Retriever + Generator

To overcome the disadvantages of generative closed-book solutions, several systems have been developed that share a similar idea:
* use a Retriever üîé to collect to collect passages of text relevant to the user's question
* use the non-parametric knowledge stored in text passages to influence the Answer Generation

In recent years, various systems have been proposed which combine the two components.
For example: ORQA (Google), REALM (Google), RAG (Meta), FiD (Meta), RETRO (Deepmind).
Probably the most popular is Retrieval-Augmented Generation (RAG), proposed by Patrick Lewis et al. in 2020, which has made a comeback in recent months..


## Fusion-in-Decoder (FiD)

![Fusion-in-Decoder](../images/fid.png)

This system is not that famous, but it is simple and effective.
It was  introduced by Gautier Izacard and Edouard Grave (Meta Research) in 2021.

- for **Retrieval** from Wikipedia, the authors considered two methods: BM25 (sparse retrieval) and Dense Passage Retrieval. *Since the retriever is not trained, FiD is potentially compatible with any retrieval system.*

- the **generative** model is based on a sequence-to-sequence network, pretrained on unsupervised data, such as T5 (transformer with encoder-decoder architecture). Each retrieved passage and its title are concatenated with the question and processed independently from other passages by the encoder. Then the decoder performs attention over the concatenation of the resulting representations of all the retrieved passages (*Fusion-in-Decoder*).
  
**Experiments and results:**
- FiD system has been trained and evaluated on 3 different QA datasets
- while conceptually simple, trained models are competitive with or better than closed book approaches and result in much smaller sizes
- major performance improvements are achieved by using the knowledge retrieved and scaling to a large number of jointly processed passages

## A lesson to take home
- (Large) Language Models üß† have strong text comprehension/generation skills 
- their knowledge is generic and is not easily updated over time
- When building NLP applications, we can combine LM with üîé Retrieval systems to provide new/specific knowledge and make them answer factually!

  
## Resources
- [Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks](https://arxiv.org/abs/2005.11401): paper introducing RAG by Patrick Lewis et al.
- [Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering](https://arxiv.org/abs/2007.01282): paper introducing Fusion-in-Decoder
- [fastRAG](https://github.com/IntelLabs/fastRAG): recent framework for efficient Retrieval Augmentation and Generation by 
Intel Labs
- [The Illustrated Retrieval Transformer](https://jalammar.github.io/illustrated-retrieval-transformer/): deep blogpost by Jay Alammar, who explains DeepMind's RETRO and Retrieval-based Answer Generation
